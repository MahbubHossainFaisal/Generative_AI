{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02a9afd1",
   "metadata": {},
   "source": [
    "### Topic: Vector Stores\n",
    "\n",
    "Vector Stores are a critical component in modern AI systems, especially when working with Large Language Models (LLMs) and techniques like **Retrieval-Augmented Generation (RAG)**. They allow us to store, retrieve, and search for information efficiently by representing text as numerical vectors (embeddings). Let’s dive into why Vector Stores are needed, how they work, and the problems they solve.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Why are Vector Stores Needed?**\n",
    "\n",
    "### **Problem: Handling Large Datasets**\n",
    "- LLMs have a limited context window (e.g., 4k, 8k, or 32k tokens), so they cannot process large documents or datasets in one go.\n",
    "- Searching through large datasets for relevant information is computationally expensive and slow.\n",
    "\n",
    "### **Solution: Vector Stores**\n",
    "- Vector Stores allow us to:\n",
    "  1. **Store embeddings**: Convert text into numerical vectors (embeddings) and store them efficiently.\n",
    "  2. **Retrieve relevant information**: Use similarity search to find the most relevant embeddings for a given query.\n",
    "  3. **Scale to large datasets**: Handle millions of embeddings efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Benefits of Vector Stores**\n",
    "\n",
    "1. **Efficient Retrieval**:\n",
    "   - Vector Stores enable fast and accurate retrieval of relevant information using similarity search.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - They can handle large datasets with millions of embeddings.\n",
    "\n",
    "3. **Context-Aware Search**:\n",
    "   - By using embeddings, Vector Stores can find semantically similar text, even if the exact keywords don’t match.\n",
    "\n",
    "4. **Integration with LLMs**:\n",
    "   - Vector Stores are essential for techniques like RAG, where retrieved embeddings are fed into LLMs to generate responses.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How Do Vector Stores Work?**\n",
    "\n",
    "### **Step 1: Generate Embeddings**\n",
    "- Text is converted into numerical vectors (embeddings) using an **embedding model** (e.g., OpenAI's `text-embedding-ada-002`).\n",
    "- Embeddings capture the semantic meaning of the text.\n",
    "\n",
    "### **Step 2: Store Embeddings**\n",
    "- Embeddings are stored in a **Vector Store** (e.g., Chroma, FAISS, Pinecone).\n",
    "- Each embedding is associated with metadata (e.g., the original text, source document, page number).\n",
    "\n",
    "### **Step 3: Perform Similarity Search**\n",
    "- When a user queries the system, the query is also converted into an embedding.\n",
    "- The Vector Store performs a **similarity search** to find the most relevant embeddings (and their associated text).\n",
    "\n",
    "### **Step 4: Retrieve and Use Information**\n",
    "- The retrieved embeddings (and their associated text) are fed into an LLM to generate a response.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. What Problem Does It Solve?**\n",
    "\n",
    "### **Scenario: Building a Document-Based Q&A System**\n",
    "Imagine you’re building a Q&A system that answers questions based on a large document (e.g., a 100-page PDF). Here’s how Vector Stores solve key problems:\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 1: Searching Large Documents**\n",
    "- Without Vector Stores, searching through a 100-page PDF for relevant information would be slow and inefficient.\n",
    "\n",
    "### **Solution: Vector Stores**\n",
    "- Convert the PDF into embeddings and store them in a Vector Store.\n",
    "- Use similarity search to quickly find the most relevant chunks for a given query.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 2: Context-Aware Retrieval**\n",
    "- Keyword-based search might miss relevant information if the exact keywords aren’t present.\n",
    "\n",
    "### **Solution: Semantic Search**\n",
    "- Vector Stores use embeddings to perform **semantic search**, which finds text that is semantically similar to the query, even if the exact keywords don’t match.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 3: Scalability**\n",
    "- Handling large datasets (e.g., millions of documents) is computationally expensive.\n",
    "\n",
    "### **Solution: Efficient Storage and Retrieval**\n",
    "- Vector Stores are optimized for storing and retrieving embeddings, making them scalable to large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Chroma Vector Database**\n",
    "\n",
    "### **What is Chroma?**\n",
    "- **Chroma** is an open-source Vector Store designed for storing and retrieving embeddings.\n",
    "- It is lightweight, easy to use, and integrates well with LangChain.\n",
    "\n",
    "### **Example: Using Chroma with LangChain**\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music. \n",
    "It works by learning patterns from existing data and using those patterns to generate new, similar data.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Split the text into chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "# Step 2: Generate embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Step 3: Store embeddings in Chroma\n",
    "vector_store = Chroma.from_texts(chunks, embeddings)\n",
    "\n",
    "# Step 4: Perform similarity search\n",
    "query = \"What is generative AI?\"\n",
    "results = vector_store.similarity_search(query, k=2)  # Retrieve top 2 results\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music.\n",
    "It works by learning patterns from existing data and using those patterns to generate new, similar data.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Similarity Search**\n",
    "\n",
    "### **What is Similarity Search?**\n",
    "- Similarity search is the process of finding the most relevant embeddings for a given query.\n",
    "- It works by comparing the **cosine similarity** between the query embedding and the stored embeddings.\n",
    "\n",
    "### **How Does It Work?**\n",
    "1. The query is converted into an embedding using the same embedding model.\n",
    "2. The Vector Store calculates the cosine similarity between the query embedding and all stored embeddings.\n",
    "3. The embeddings with the highest similarity scores are returned.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Benefits of Similarity Search**\n",
    "\n",
    "1. **Semantic Understanding**:\n",
    "   - Finds text that is semantically similar to the query, even if the exact keywords don’t match.\n",
    "\n",
    "2. **Efficient Retrieval**:\n",
    "   - Quickly retrieves the most relevant information from large datasets.\n",
    "\n",
    "3. **Context-Aware Responses**:\n",
    "   - Ensures that the LLM receives relevant context for generating accurate responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5676a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffcd4069",
   "metadata": {},
   "source": [
    "### **Step 1: Set Up Hugging Face API**\n",
    "\n",
    "#### **1.1 Create a Hugging Face Account**\n",
    "1. Go to [Hugging Face](https://huggingface.co/) and create an account.\n",
    "2. Navigate to your profile settings and generate a new **Access Token** under the **Tokens** section.\n",
    "\n",
    "#### **1.2 Store the Access Token in `.env`**\n",
    "Open the `.env` file in your project’s root folder and add the following line:\n",
    "\n",
    "```plaintext\n",
    "HUGGINGFACEHUB_ACCESS_TOKEN=\"<your_access_token>\"\n",
    "```\n",
    "\n",
    "- **Explanation**: This securely stores your Hugging Face access token, which will be used to authenticate API requests.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Create a File for Hugging Face Chat Model**\n",
    "\n",
    "Under the `ChatModels` folder, create a new file named `chatmodel_hf_api.py`.\n",
    "\n",
    "```bash\n",
    "touch ChatModels/chatmodel_hf_api.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Write the Code**\n",
    "\n",
    "Open the `chatmodel_hf_api.py` file and add the following code:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the Hugging Face model\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-chat-v1.0\",\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "# Wrap the Hugging Face model in a ChatHuggingFace object\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"Explain the concept of blockchain technology in simple terms.\"\n",
    "\n",
    "# Invoke the model with the prompt\n",
    "result = model.invoke(prompt)\n",
    "\n",
    "# Print the result\n",
    "print(result.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Understand the Code**\n",
    "\n",
    "Let’s break down the code step by step:\n",
    "\n",
    "#### **4.1 Import Libraries**\n",
    "- **`ChatHuggingFace`**: This is the class provided by LangChain to interact with Hugging Face chat models.\n",
    "- **`HuggingFaceEndpoint`**: This is used to configure the Hugging Face model endpoint.\n",
    "- **`load_dotenv`**: This function loads environment variables from the `.env` file.\n",
    "\n",
    "#### **4.2 Load Environment Variables**\n",
    "- **`load_dotenv()`**: This ensures that the `HUGGINGFACEHUB_ACCESS_TOKEN` is available in the environment.\n",
    "\n",
    "#### **4.3 Configure the Hugging Face Model**\n",
    "- **`HuggingFaceEndpoint(repo_id=\"TinyLlama/TinyLlama-1.1B-chat-v1.0\", task=\"text-generation\")`**: This initializes the TinyLlama model hosted on Hugging Face. The `repo_id` specifies the model, and the `task` defines the type of task (e.g., text-generation).\n",
    "\n",
    "#### **4.4 Wrap the Model in a ChatHuggingFace Object**\n",
    "- **`ChatHuggingFace(llm=llm)`**: This wraps the Hugging Face model in a LangChain-compatible chat model.\n",
    "\n",
    "#### **4.5 Define the Prompt**\n",
    "- **`prompt = \"Explain the concept of blockchain technology in simple terms.\"`**: This is the input text that will be sent to the model.\n",
    "\n",
    "#### **4.6 Invoke the Model**\n",
    "- **`model.invoke(prompt)`**: This sends the prompt to the model and retrieves the response.\n",
    "\n",
    "#### **4.7 Print the Result**\n",
    "- **`print(result.content)`**: The response from the model is stored in `result.content`, which is then printed to the console.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Run the Script**\n",
    "\n",
    "Execute the script to see the model’s response.\n",
    "\n",
    "```bash\n",
    "python ChatModels/chatmodel_hf_api.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Expected Output**\n",
    "\n",
    "When you run the script, you should see an output similar to the following:\n",
    "\n",
    "```plaintext\n",
    "Blockchain is a type of digital ledger that records transactions in a secure and transparent way. It consists of a chain of blocks, where each block contains a list of transactions. Once a block is added to the chain, it cannot be altered, making the data immutable. This technology is the foundation of cryptocurrencies like Bitcoin and is also used in various other applications like supply chain management and voting systems.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Customize the Prompt**\n",
    "\n",
    "You can experiment with different prompts to see how the model responds. For example:\n",
    "\n",
    "```python\n",
    "prompt = \"Write a short poem about the stars.\"\n",
    "```\n",
    "\n",
    "Run the script again, and you’ll get a creative poem generated by the TinyLlama model!\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Additional Tips**\n",
    "\n",
    "1. **Model Selection**: Hugging Face hosts thousands of models. You can replace `TinyLlama/TinyLlama-1.1B-chat-v1.0` with any other model ID from the Hugging Face Model Hub.\n",
    "\n",
    "2. **Task Configuration**: The `task` parameter can be adjusted based on the model’s capabilities. For example:\n",
    "   - `text-generation`: For generating text.\n",
    "   - `text-classification`: For classifying text.\n",
    "   - `question-answering`: For answering questions.\n",
    "\n",
    "3. **Error Handling**: Always include error handling to manage API rate limits or connectivity issues. For example:\n",
    "   ```python\n",
    "   try:\n",
    "       result = model.invoke(prompt)\n",
    "       print(result.content)\n",
    "   except Exception as e:\n",
    "       print(f\"An error occurred: {e}\")\n",
    "   ```\n",
    "\n",
    "4. **Model Parameters**: You can customize the model’s behavior by adding parameters like `temperature` (controls randomness) and `max_tokens` (limits response length). For example:\n",
    "   ```python\n",
    "   llm = HuggingFaceEndpoint(\n",
    "       repo_id=\"TinyLlama/TinyLlama-1.1B-chat-v1.0\",\n",
    "       task=\"text-generation\",\n",
    "       temperature=0.7,\n",
    "       max_tokens=200\n",
    "   )\n",
    "   ```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**\n",
    "- Explore other models on Hugging Face, such as **GPT-Neo**, **BLOOM**, or **Falcon**.\n",
    "- Integrate Hugging Face models into a larger application, such as a chatbot or a content generation tool.\n",
    "- Compare the performance of different open-source models with proprietary models like OpenAI’s GPT or Anthropic’s Claude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282380d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7beadfde",
   "metadata": {},
   "source": [
    "### 1. **Sample Chain**\n",
    "A **chain** in Generative AI refers to a sequence of actions (or steps) that process input data and produce output data. Chains are used to build workflows where each step can be a model, a parser, or any other processing unit.\n",
    "\n",
    "#### Traditional Chains vs LCEL Chains\n",
    "- **Traditional Chains**: These are the older way of building chains in LangChain. They are still supported but are considered **legacy**. They are less flexible and harder to use for advanced workflows.\n",
    "- **LCEL Chains (LangChain Expression Language)**: This is the newer, more compact, and powerful way to build chains. LCEL chains are easier to write, debug, and extend.\n",
    "\n",
    "**Why LCEL is favored over Traditional Chains:**\n",
    "1. **Compact Syntax**: LCEL chains are more concise and easier to read.\n",
    "2. **Advanced Functionality**: LCEL supports advanced features like streaming, batching, and async operations out of the box.\n",
    "3. **Better Debugging**: LCEL provides better error handling and debugging capabilities.\n",
    "\n",
    "**Example of Traditional Chain vs LCEL Chain:**\n",
    "```python\n",
    "# Traditional Chain\n",
    "from langchain import LLMChain, PromptTemplate, OpenAI\n",
    "\n",
    "prompt = PromptTemplate(template=\"Tell me a joke about {topic}\", input_variables=[\"topic\"])\n",
    "llm = OpenAI()\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run(\"programming\")\n",
    "print(response)\n",
    "\n",
    "# LCEL Chain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate(template=\"Tell me a joke about {topic}\", input_variables=[\"topic\"])\n",
    "llm = OpenAI()\n",
    "chain = prompt | llm  # Compact syntax using LCEL\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **What StrOutputParser Does**\n",
    "The **StrOutputParser** is a utility in LangChain that converts the output of a language model (LLM) into a string format. \n",
    "\n",
    "#### Why Use StrOutputParser?\n",
    "Even though LLMs provide responses in text, the output might be in a complex format (e.g., JSON, lists, etc.). StrOutputParser ensures the output is always a clean, usable string.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(template=\"Tell me a joke about {topic}\", input_variables=[\"topic\"])\n",
    "llm = OpenAI()\n",
    "chain = prompt | llm | StrOutputParser()  # LCEL chain with StrOutputParser\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)\n",
    "```\n",
    "Here, the `StrOutputParser` ensures the output is a plain string, even if the LLM returns a more complex structure.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Intro to LCEL**\n",
    "**LCEL (LangChain Expression Language)** is the newest way to build chains in LangChain. It provides a compact and expressive syntax for defining workflows.\n",
    "\n",
    "#### Key Features of LCEL:\n",
    "1. **Compact Syntax**: Chains are defined using the `|` operator, making them easy to read and write.\n",
    "2. **Advanced Functionality**: Supports streaming, batching, and async operations.\n",
    "3. **Backward Compatibility**: Traditional chains are still supported but are treated as legacy.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Main Goals of LCEL**\n",
    "1. **Ease of Use**: Make it easy to build chains in a compact and readable way.\n",
    "2. **Advanced Functionality**: Support advanced features like streaming, batching, and async operations.\n",
    "3. **Debugging and Error Handling**: Provide better tools for debugging and error handling.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Example of Legacy Chain vs LCEL**\n",
    "Letâ€™s compare a legacy chain with an LCEL chain:\n",
    "\n",
    "**Legacy Chain:**\n",
    "```python\n",
    "from langchain import LLMChain, PromptTemplate, OpenAI\n",
    "\n",
    "prompt = PromptTemplate(template=\"Tell me a joke about {topic}\", input_variables=[\"topic\"])\n",
    "llm = OpenAI()\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.run(\"programming\")\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**LCEL Chain:**\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "prompt = PromptTemplate(template=\"Tell me a joke about {topic}\", input_variables=[\"topic\"])\n",
    "llm = OpenAI()\n",
    "chain = prompt | llm  # Compact syntax using LCEL\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "- LCEL chains are more compact and easier to read.\n",
    "- LCEL chains support advanced features like streaming and async operations.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Runnables**\n",
    "In LCEL, every component (e.g., prompt, LLM, parser) is a **Runnable**. A Runnable is an object that can be executed as part of a chain.\n",
    "\n",
    "#### Runnable Execution Order in LCEL Chain:\n",
    "1. **Prompt**: Takes input and generates a prompt.\n",
    "2. **LLM**: Processes the prompt and generates a response.\n",
    "3. **Parser**: Converts the LLM output into a usable format.\n",
    "\n",
    "**Diagram:**\n",
    "```\n",
    "Input -> Prompt -> LLM -> Parser -> Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **LCEL Chains/Runnables Usage**\n",
    "LCEL chains can be executed in different ways:\n",
    "1. **`chain.invoke()`**: Executes the chain synchronously for a single input.\n",
    "2. **`chain.stream()`**: Streams the output in real-time (useful for long responses).\n",
    "3. **`chain.batch()`**: Executes the chain for multiple inputs in parallel.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Single Input\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "\n",
    "# Streaming\n",
    "for chunk in chain.stream({\"topic\": \"programming\"}):\n",
    "    print(chunk)\n",
    "\n",
    "# Batch Processing\n",
    "responses = chain.batch([{\"topic\": \"programming\"}, {\"topic\": \"AI\"}])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Asynchronous Behavior**\n",
    "Asynchronous execution allows you to run chains without blocking the main thread. This is useful for handling multiple requests simultaneously.\n",
    "\n",
    "#### Asynchronous Methods:\n",
    "1. **`chain.ainvoke()`**: Executes the chain asynchronously for a single input.\n",
    "2. **`chain.astream()`**: Streams the output asynchronously.\n",
    "3. **`chain.abatch()`**: Executes the chain asynchronously for multiple inputs.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "# Single Input\n",
    "response = await chain.ainvoke({\"topic\": \"programming\"})\n",
    "\n",
    "# Streaming\n",
    "async for chunk in chain.astream({\"topic\": \"programming\"}):\n",
    "    print(chunk)\n",
    "\n",
    "# Batch Processing\n",
    "responses = await chain.abatch([{\"topic\": \"programming\"}, {\"topic\": \"AI\"}])\n",
    "```\n",
    "\n",
    "**Key Differences:**\n",
    "- Asynchronous methods use `await` and are non-blocking.\n",
    "- They are ideal for handling multiple requests or long-running tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table: LCEL Methods\n",
    "| Method         | Description                          | Sync/Async |\n",
    "|----------------|--------------------------------------|------------|\n",
    "| `invoke()`     | Executes chain for single input      | Sync       |\n",
    "| `stream()`     | Streams output in real-time          | Sync       |\n",
    "| `batch()`      | Executes chain for multiple inputs   | Sync       |\n",
    "| `ainvoke()`    | Executes chain asynchronously        | Async      |\n",
    "| `astream()`    | Streams output asynchronously        | Async      |\n",
    "| `abatch()`     | Executes chain asynchronously (batch)| Async      |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4541a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

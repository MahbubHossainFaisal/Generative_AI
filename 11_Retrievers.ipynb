{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5e6997",
   "metadata": {},
   "source": [
    "### Topic: Retrievers\n",
    "\n",
    "Retrievers are a key component in modern AI systems, especially when working with **Large Language Models (LLMs)** and techniques like **Retrieval-Augmented Generation (RAG)**. They allow us to efficiently retrieve relevant information from large datasets or knowledge bases. Let’s dive into why retrievers are needed, how they work, and the problems they solve.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Why are Retrievers Needed?**\n",
    "\n",
    "### **Problem: Searching Large Datasets**\n",
    "- LLMs have a limited context window (e.g., 4k, 8k, or 32k tokens), so they cannot process large datasets in one go.\n",
    "- Searching through large datasets for relevant information is computationally expensive and slow.\n",
    "\n",
    "### **Solution: Retrievers**\n",
    "- Retrievers allow us to:\n",
    "  1. **Efficiently retrieve relevant information**: Fetch only the most relevant chunks of text for a given query.\n",
    "  2. **Scale to large datasets**: Handle millions of documents or embeddings.\n",
    "  3. **Integrate with LLMs**: Feed retrieved information into LLMs to generate accurate and context-aware responses.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Benefits of Retrievers**\n",
    "\n",
    "1. **Efficient Retrieval**:\n",
    "   - Retrievers enable fast and accurate retrieval of relevant information.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - They can handle large datasets with millions of documents or embeddings.\n",
    "\n",
    "3. **Context-Aware Search**:\n",
    "   - Retrievers use embeddings to perform **semantic search**, which finds text that is semantically similar to the query, even if the exact keywords don’t match.\n",
    "\n",
    "4. **Integration with LLMs**:\n",
    "   - Retrievers are essential for techniques like RAG, where retrieved information is fed into LLMs to generate responses.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How Do Retrievers Work?**\n",
    "\n",
    "### **Step 1: Generate Embeddings**\n",
    "- Text is converted into numerical vectors (embeddings) using an **embedding model** (e.g., OpenAI's `text-embedding-ada-002`).\n",
    "\n",
    "### **Step 2: Store Embeddings**\n",
    "- Embeddings are stored in a **Vector Store** (e.g., Chroma, FAISS, Pinecone).\n",
    "\n",
    "### **Step 3: Perform Retrieval**\n",
    "- When a user queries the system, the query is converted into an embedding.\n",
    "- The retriever performs a **similarity search** to find the most relevant embeddings (and their associated text).\n",
    "\n",
    "### **Step 4: Retrieve and Use Information**\n",
    "- The retrieved embeddings (and their associated text) are fed into an LLM to generate a response.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. What Problem Does It Solve?**\n",
    "\n",
    "### **Scenario: Building a Document-Based Q&A System**\n",
    "Imagine you’re building a Q&A system that answers questions based on a large document (e.g., a 100-page PDF). Here’s how retrievers solve key problems:\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 1: Searching Large Documents**\n",
    "- Without retrievers, searching through a 100-page PDF for relevant information would be slow and inefficient.\n",
    "\n",
    "### **Solution: Retrievers**\n",
    "- Convert the PDF into embeddings and store them in a Vector Store.\n",
    "- Use a retriever to quickly find the most relevant chunks for a given query.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 2: Context-Aware Retrieval**\n",
    "- Keyword-based search might miss relevant information if the exact keywords aren’t present.\n",
    "\n",
    "### **Solution: Semantic Search**\n",
    "- Retrievers use embeddings to perform **semantic search**, which finds text that is semantically similar to the query, even if the exact keywords don’t match.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 3: Scalability**\n",
    "- Handling large datasets (e.g., millions of documents) is computationally expensive.\n",
    "\n",
    "### **Solution: Efficient Retrieval**\n",
    "- Retrievers are optimized for retrieving relevant information from large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Vector Stores vs Retrievers**\n",
    "\n",
    "Let’s compare **Vector Stores** and **Retrievers** in terms of purpose, functionality, storage, and flexibility.\n",
    "\n",
    "| **Aspect**              | **Vector Stores**                                                                 | **Retrievers**                                                                 |\n",
    "|--------------------------|-----------------------------------------------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| **Purpose and Functionality** | Store embeddings and perform similarity search.                                   | Retrieve relevant information from Vector Stores and integrate with LLMs.      |\n",
    "| **Storage and Retrieval**    | Store embeddings and metadata (e.g., text, source document).                     | Fetch embeddings and associated text from Vector Stores.                       |\n",
    "| **Flexibility**              | Can store embeddings from various sources (e.g., text, images, audio).            | Can be customized to retrieve specific types of information (e.g., by metadata).|\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Differences Between `similarity_search` and `as_retriever`**\n",
    "\n",
    "### **`similarity_search`**\n",
    "- **Purpose**: Performs a similarity search on a Vector Store to find the most relevant embeddings for a given query.\n",
    "- **Output**: Returns a list of documents or embeddings.\n",
    "- **Use Case**: Ideal for direct retrieval of relevant information.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "results = vector_store.similarity_search(query, k=2)  # Retrieve top 2 results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **`as_retriever`**\n",
    "- **Purpose**: Converts a Vector Store into a retriever object that can be used in a chain or pipeline.\n",
    "- **Output**: Returns a retriever object that can fetch relevant information.\n",
    "- **Use Case**: Ideal for integrating retrieval into a larger workflow (e.g., RAG).\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "retriever = vector_store.as_retriever()\n",
    "results = retriever.get_relevant_documents(query)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Example: Using Retrievers with LangChain**\n",
    "\n",
    "Let’s say you have the following text:\n",
    "\n",
    "```\n",
    "Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music. \n",
    "It works by learning patterns from existing data and using those patterns to generate new, similar data.\n",
    "```\n",
    "\n",
    "You want to convert this text into embeddings, store them in a Vector Store, and use a retriever to fetch relevant information.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Generate Embeddings**\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Generate embeddings for the text\n",
    "text = \"Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music.\"\n",
    "embedding = embeddings.embed_query(text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Store Embeddings in a Vector Store**\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Store embeddings in Chroma\n",
    "vector_store = Chroma.from_texts([text], embeddings)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Use a Retriever**\n",
    "```python\n",
    "# Convert Vector Store into a retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Perform retrieval\n",
    "query = \"What is generative AI?\"\n",
    "results = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Benefits of Retrievers**\n",
    "\n",
    "1. **Efficient Retrieval**:\n",
    "   - Retrievers enable fast and accurate retrieval of relevant information.\n",
    "\n",
    "2. **Scalability**:\n",
    "   - They can handle large datasets with millions of documents or embeddings.\n",
    "\n",
    "3. **Context-Aware Search**:\n",
    "   - Retrievers use embeddings to perform **semantic search**, which finds text that is semantically similar to the query, even if the exact keywords don’t match.\n",
    "\n",
    "4. **Integration with LLMs**:\n",
    "   - Retrievers are essential for techniques like RAG, where retrieved information is fed into LLMs to generate responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096497ee",
   "metadata": {},
   "source": [
    "### Topic: Top-k Retrieval\n",
    "\n",
    "In the context of **Retrieval-Augmented Generation (RAG)** and other AI systems, **Top-k retrieval** is a technique used to control how many relevant embeddings or documents are retrieved to build the answer to a user's query. The \"k\" in Top-k refers to the number of items (e.g., embeddings, documents) that are retrieved from a dataset or knowledge base. Let’s dive into why Top-k is important, how it works, and how it helps improve results.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is Top-k Retrieval?**\n",
    "\n",
    "### **Definition**:\n",
    "- **Top-k retrieval** refers to fetching the **k most relevant items** (e.g., embeddings, documents) from a dataset or knowledge base for a given query.\n",
    "- The value of **k** is a hyperparameter that you can adjust based on your use case.\n",
    "\n",
    "### **Example**:\n",
    "- If **k = 3**, the system retrieves the **3 most relevant embeddings** or documents for a query.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why is Top-k Retrieval Needed?**\n",
    "\n",
    "### **Problem: Information Overload**\n",
    "- Without Top-k retrieval, the system might retrieve too many irrelevant or redundant items, leading to:\n",
    "  - **Increased computational cost**: Processing more items than necessary.\n",
    "  - **Reduced accuracy**: Irrelevant items can dilute the quality of the final response.\n",
    "\n",
    "### **Solution: Top-k Retrieval**\n",
    "- By limiting the number of retrieved items to the **k most relevant ones**, the system can:\n",
    "  - Focus on the most important information.\n",
    "  - Reduce computational overhead.\n",
    "  - Improve the accuracy and relevance of the final response.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Importance of Top-k Retrieval: A Scenario**\n",
    "\n",
    "### **Scenario: Building a Document-Based Q&A System**\n",
    "Imagine you’re building a Q&A system that answers questions based on a large document (e.g., a 100-page PDF). Here’s how Top-k retrieval solves key problems:\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 1: Retrieving Too Many Irrelevant Items**\n",
    "- Without Top-k retrieval, the system might retrieve hundreds of embeddings or documents, many of which are irrelevant to the query.\n",
    "\n",
    "### **Solution: Limit Retrieval to Top-k Items**\n",
    "- Use Top-k retrieval to fetch only the **k most relevant embeddings** or documents.\n",
    "- For example, if **k = 5**, the system retrieves only the **5 most relevant chunks** of text.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 2: Balancing Relevance and Efficiency**\n",
    "- Retrieving too few items might miss important information, while retrieving too many can overwhelm the system.\n",
    "\n",
    "### **Solution: Adjust k Based on Use Case**\n",
    "- Choose an appropriate value for **k** based on the complexity of the query and the size of the dataset.\n",
    "- For example:\n",
    "  - Use **k = 3** for simple queries.\n",
    "  - Use **k = 10** for complex queries that require more context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem 3: Improving Response Quality**\n",
    "- Irrelevant or redundant items can reduce the quality of the final response.\n",
    "\n",
    "### **Solution: Focus on Top-k Items**\n",
    "- By focusing on the **k most relevant items**, the system can generate more accurate and concise responses.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. How Does Top-k Retrieval Work?**\n",
    "\n",
    "### **Step 1: Generate Embeddings**\n",
    "- Convert the query and the documents into embeddings using an **embedding model**.\n",
    "\n",
    "### **Step 2: Perform Similarity Search**\n",
    "- Use a **Vector Store** to perform a similarity search and rank the embeddings based on their relevance to the query.\n",
    "\n",
    "### **Step 3: Retrieve Top-k Items**\n",
    "- Fetch the **k most relevant embeddings** or documents.\n",
    "\n",
    "### **Step 4: Generate the Final Response**\n",
    "- Feed the retrieved items into an LLM to generate the final response.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Example: Using Top-k Retrieval with LangChain**\n",
    "\n",
    "Let’s say you have the following text:\n",
    "\n",
    "```\n",
    "Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music. \n",
    "It works by learning patterns from existing data and using those patterns to generate new, similar data.\n",
    "```\n",
    "\n",
    "You want to retrieve the **top 2 most relevant chunks** for the query: *\"What is generative AI?\"*\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Generate Embeddings**\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Generate embeddings for the text\n",
    "text = \"Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music.\"\n",
    "embedding = embeddings.embed_query(text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Store Embeddings in a Vector Store**\n",
    "```python\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Store embeddings in Chroma\n",
    "vector_store = Chroma.from_texts([text], embeddings)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Perform Top-k Retrieval**\n",
    "```python\n",
    "# Perform Top-k retrieval (k=2)\n",
    "query = \"What is generative AI?\"\n",
    "results = vector_store.similarity_search(query, k=2)  # Retrieve top 2 results\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. How Top-k Retrieval Helps Us Get Better Results**\n",
    "\n",
    "1. **Focuses on Relevance**:\n",
    "   - By retrieving only the **k most relevant items**, the system avoids irrelevant or redundant information.\n",
    "\n",
    "2. **Improves Efficiency**:\n",
    "   - Reduces computational overhead by limiting the number of items to process.\n",
    "\n",
    "3. **Enhances Response Quality**:\n",
    "   - Ensures that the LLM receives only the most relevant context, leading to more accurate and concise responses.\n",
    "\n",
    "4. **Customizable**:\n",
    "   - The value of **k** can be adjusted based on the complexity of the query and the size of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742cf36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

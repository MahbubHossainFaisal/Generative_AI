{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48824117",
   "metadata": {},
   "source": [
    "# **Tokenization – The Secret Language of AI (Explained in Detail!)**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is Tokenization? (AI’s \"Word Puzzle\")**  \n",
    "\n",
    "Imagine you're playing with **Lego blocks**, and instead of using whole words, AI breaks everything into **smaller pieces** called **tokens**.  \n",
    "\n",
    "- **Tokens** can be:  \n",
    "  - Whole words (`\"hello\"`)  \n",
    "  - Parts of words (`\"Chat\" + \"GPT\"`)  \n",
    "  - Punctuation (`\"!\"`, `\"?\"`)  \n",
    "  - Even spaces sometimes!  \n",
    "\n",
    "**Example:**  \n",
    "- *\"I love AI!\"* → `[\"I\", \" love\", \" AI\", \"!\"]` (**4 tokens**)  \n",
    "- *\"ChatGPT is cool\"* → `[\"Chat\", \"G\", \"PT\", \" is\", \" cool\"]` (**5 tokens**)  \n",
    "\n",
    "🔹 **Why?** AI works with numbers, not words. Tokenization converts text into numbers for processing.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Token Limits – AI’s \"Memory Wall\"**  \n",
    "\n",
    "Every AI model has a **maximum token limit** (like a brain with limited space).  \n",
    "\n",
    "| Model          | Max Tokens | What Happens if Exceeded? |  \n",
    "|----------------|-----------|--------------------------|  \n",
    "| GPT-3.5        | ~4,096    | Cuts off old text (like a goldfish!) |  \n",
    "| GPT-4          | ~32,000   | Still forgets, but much later |  \n",
    "| Claude 3       | ~200,000  | Handles books, but slows down |  \n",
    "\n",
    "**Real-World Impact:**  \n",
    "- If you paste a **long article**, AI might **ignore the first half**.  \n",
    "- Code, documents, and conversations all **consume tokens**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. How to Check Tokens in Python (The Detective Tool)**  \n",
    "\n",
    "Want to see how many tokens your text uses? Here’s the magic code:  \n",
    "\n",
    "```python\n",
    "# Install the library first (if needed):  \n",
    "# pip install transformers  \n",
    "\n",
    "from transformers import GPT2Tokenizer  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  \n",
    "\n",
    "text = \"Your long text goes here. How many tokens?\"  \n",
    "tokens = tokenizer.encode(text)  \n",
    "\n",
    "print(\"Total tokens used:\", len(tokens))  \n",
    "```\n",
    "\n",
    "**Try it!**  \n",
    "- `\"Hello, world!\"` → **3 tokens**  \n",
    "- A 1000-word essay → **~1500 tokens**  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. How to Avoid Hitting Token Limits (Pro Tricks!)**  \n",
    "\n",
    "### **① Shortening the Prompt**  \n",
    "- **Bad:** *\"Explain quantum physics in extreme detail with examples, analogies, and historical context...\"*  \n",
    "- **Good:** *\"Summarize quantum physics in 3 sentences.\"*  \n",
    "\n",
    "### **② Chunking – Breaking Big Texts**  \n",
    "- If you have a **100-page PDF**, split it into **10-page sections** and process one at a time.  \n",
    "\n",
    "### **③ Windowed Chunks – The \"Sliding Memory\" Trick**  \n",
    "- Process text in **overlapping chunks** to avoid losing context.  \n",
    "  - Example:  \n",
    "    - Chunk 1: Pages 1-10  \n",
    "    - Chunk 2: Pages 8-18 (keeps some overlap)  \n",
    "\n",
    "### **④ Summarization – AI’s \"TL;DR\" Mode**  \n",
    "- First, ask AI to **summarize** a long document.  \n",
    "- Then, feed the **summary** instead of the full text.  \n",
    "\n",
    "**Example Workflow:**  \n",
    "1. *\"Summarize this 10-page report in 5 bullet points.\"*  \n",
    "2. *\"Now, analyze this summary for key trends.\"*  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454aa396",
   "metadata": {},
   "source": [
    "# **Log Probabilities & Temperature – AI’s \"Creativity Control Panel\"**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. What Are Log Probabilities? (AI’s \"Confidence Meter\")**  \n",
    "\n",
    "When AI generates text, it doesn’t just *guess* words—it **calculates probabilities** for every possible next word.  \n",
    "\n",
    "- **Log probabilities** = A math-friendly way to measure how **confident** the AI is about each choice.  \n",
    "  - Higher log prob → More likely/correct word.  \n",
    "  - Lower log prob → Less likely/risky word.  \n",
    "\n",
    "### **Example:**  \n",
    "*Input:* `\"The sky is ___\"`  \n",
    "*Possible outputs:*  \n",
    "- `\"blue\"` (log prob = -0.2 → **90% confidence**)  \n",
    "- `\"green\"` (log prob = -1.5 → **20% confidence**)  \n",
    "- `\"spicy\"` (log prob = -5.0 → **0.1% confidence**)  \n",
    "\n",
    "🔹 **Why \"log\"?**  \n",
    "- Probabilities are tiny numbers (e.g., 0.0001), so we use **logarithms** to make them easier to work with.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. What is the Temperature Parameter? (AI’s \"Creativity Dial\")**  \n",
    "\n",
    "Temperature controls **how random vs. predictable** the AI’s responses are.  \n",
    "\n",
    "| **Temperature** | **Effect on AI** | **Best For** |  \n",
    "|----------------|----------------|-------------|  \n",
    "| **Low (0.1 - 0.3)** | Plays it safe, picks the **most likely** words. | Factual answers, code, summaries. |  \n",
    "| **Medium (0.5 - 0.7)** | Balanced—some creativity, some accuracy. | Casual chat, brainstorming. |  \n",
    "| **High (0.8 - 1.2+)** | Wildly creative, takes big risks. | Poetry, jokes, fiction. |  \n",
    "\n",
    "### **Real-World Examples:**  \n",
    "- **Temp = 0.2** → *\"The capital of France is Paris.\"* (Boring but correct.)  \n",
    "- **Temp = 0.8** → *\"The capital of France? A croissant-filled dream called Paris!\"* (Fun but risky.)  \n",
    "- **Temp = 1.5** → *\"France’s capital? Probably a baguette.\"* (Hallucination mode.)  \n",
    "\n",
    "⚠️ **Warning:** High temps can lead to **nonsense or false info!**  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. What Happens When You Change Temperature?**  \n",
    "\n",
    "### **Increasing Temperature (↑ Creativity, ↓ Accuracy)**  \n",
    "- AI **explores less likely words** (more surprising, fun, or weird).  \n",
    "- Good for:  \n",
    "  - Creative writing  \n",
    "  - Jokes & storytelling  \n",
    "  - Brainstorming ideas  \n",
    "\n",
    "### **Decreasing Temperature (↓ Creativity, ↑ Accuracy)**  \n",
    "- AI **sticks to the safest, most probable words**.  \n",
    "- Good for:  \n",
    "  - Medical/legal advice  \n",
    "  - Code generation  \n",
    "  - Fact-based Q&A  \n",
    "\n",
    "### **Extreme Cases:**  \n",
    "- **Temp = 0** → Always picks the **#1 most likely word** (repetitive, robotic).  \n",
    "- **Temp → ∞** → Pure randomness (word salad).  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. The \"Best Of\" Parameter & Its Link to Temperature**  \n",
    "\n",
    "### **What is \"Best Of\"?**  \n",
    "- When you set `best_of=5`, the AI **generates 5 responses** and picks the **best one** (based on log probabilities).  \n",
    "\n",
    "### **How It Works With Temperature:**  \n",
    "| **Temperature** | **Best Of = 3** | **Result** |  \n",
    "|----------------|----------------|-----------|  \n",
    "| Low (0.2) | All 3 responses are similar. | Picks the **safest** version. |  \n",
    "| High (1.0) | 3 very different responses. | Picks the **most interesting** one (but may be less accurate). |  \n",
    "\n",
    "### **Pro Tip:**  \n",
    "- Use `best_of` with **medium temperature** (0.5-0.7) to get **diverse but coherent** answers.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e7f0d",
   "metadata": {},
   "source": [
    "**Temperature = 0 does NOT stop the LLM from providing suggestions**, but it fundamentally changes how those suggestions are generated:\n",
    "\n",
    "### What Happens at Temperature = 0?\n",
    "1. **Deterministic Output**:\n",
    "   - The model will always select the single most probable next token (word piece) at every step\n",
    "   - This eliminates all randomness in the output\n",
    "\n",
    "2. **Still Provides Answers**:\n",
    "   - It will still respond to prompts and questions\n",
    "   - The answers will be the most statistically likely responses based on its training\n",
    "\n",
    "3. **Behavior Changes**:\n",
    "   - Responses become extremely predictable and consistent\n",
    "   - For any given prompt, you'll always get exactly the same output\n",
    "   - The output will be the \"safest\", most conventional response possible\n",
    "\n",
    "### Key Implications:\n",
    "- **Not \"stopped\"**, just maximally constrained\n",
    "- The model still uses its full knowledge\n",
    "- You'll get the most statistically likely answer every time\n",
    "- Useful when you need 100% reproducible results\n",
    "\n",
    "### Example:\n",
    "Prompt: \"Tell me a joke about computers\"\n",
    "\n",
    "- Temp > 0: Might generate different jokes each time\n",
    "- Temp = 0: Will always output the single most statistically common computer joke in its training data\n",
    "\n",
    "### When to Use Temp = 0:\n",
    "- Testing/benchmarking\n",
    "- When you need identical outputs from identical inputs\n",
    "- For maximally conservative responses\n",
    "\n",
    "### When to Avoid Temp = 0:\n",
    "- For creative tasks\n",
    "- When you want varied responses\n",
    "- For brainstorming sessions\n",
    "\n",
    "The model still \"suggests\" answers - it just always suggests the single most statistically likely one at every decision point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ea5cc",
   "metadata": {},
   "source": [
    "# **AI Hallucinations – When Your AI Starts \"Making Up Stuff\"**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. What Makes AI Hallucinate? (Why AI \"Lies\")**  \n",
    "\n",
    "AI hallucinations happen when the model **confidently generates false or nonsensical information**. Here’s why it happens:  \n",
    "\n",
    "### **① Lack of Knowledge (Guessing Instead of Knowing)**  \n",
    "- If the AI wasn’t trained on enough data about a topic, it **fills gaps with plausible-sounding nonsense**.  \n",
    "  - *Example:* Asking for obscure historical facts → AI might invent fake events.  \n",
    "\n",
    "### **② Over-Optimization (Trying Too Hard to Please You)**  \n",
    "- Some models prioritize **giving a satisfying answer** over a correct one.  \n",
    "  - *Example:*  \n",
    "    - You: *\"Who invented the internet in 1600?\"*  \n",
    "    - AI (instead of saying \"nobody\"): *\"Sir Francis Bacon, using steam-powered telegraphs!\"*  \n",
    "\n",
    "### **③ Vague or Misleading Prompts**  \n",
    "- If your question is unclear, AI **misinterprets and invents details**.  \n",
    "  - *Bad Prompt:* *\"Tell me about the scientist who made cats glow.\"*  \n",
    "  - *AI Hallucination:* *\"Dr. Meowington in 1987 genetically engineered bioluminescent cats!\"*  \n",
    "\n",
    "### **④ High Temperature = More Creativity = More Risk**  \n",
    "- As we learned earlier, **high temperature** makes AI **explore unlikely answers**, increasing hallucination risk.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. How to Avoid Hallucinations (Pro Tips!)**  \n",
    "\n",
    "### **① Ask for Sources & Evidence**  \n",
    "- *Bad:* *\"Explain quantum physics.\"*  \n",
    "- *Good:* *\"Explain quantum physics and cite peer-reviewed papers.\"*  \n",
    "\n",
    "### **② Use Constraints in Prompts**  \n",
    "- *Bad:* *\"Write a biography of Nikola Tesla.\"*  \n",
    "- *Good:* *\"Write a factual biography of Nikola Tesla, only including verified events from reliable sources.\"*  \n",
    "\n",
    "### **③ Request Uncertainty Awareness**  \n",
    "- *Bad:* *\"When was the first AI created?\"*  \n",
    "- *Good:* *\"When was the first AI created? If unsure, say 'I don’t know' instead of guessing.\"*  \n",
    "\n",
    "### **④ Lower Temperature for Serious Topics**  \n",
    "- Use **temp = 0.3 or lower** for medical/legal/financial advice.  \n",
    "\n",
    "### **⑤ Fact-Check with Follow-Up Prompts**  \n",
    "- *First prompt:* *\"Who invented the telephone?\"*  \n",
    "- *Follow-up:* *\"Are you sure? Double-check your answer.\"*  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Best Anti-Hallucination Prompts (Copy-Paste Ready!)**  \n",
    "\n",
    "| **Scenario**               | **Hallucination-Prone Prompt** | **Better, Safer Prompt** |  \n",
    "|---------------------------|-------------------------------|-------------------------|  \n",
    "| **Medical Advice**        | *\"How do I treat a fever?\"*    | *\"What are medically verified ways to reduce fever in adults? Provide sources.\"* |  \n",
    "| **Historical Facts**      | *\"Tell me about ancient Rome.\"* | *\"Summarize key facts about ancient Rome from trusted history books.\"* |  \n",
    "| **Technical Explanations**| *\"Explain blockchain.\"*        | *\"Explain blockchain in simple terms, avoiding speculation.\"* |  \n",
    "| **Creative Writing**      | *\"Write a news article.\"*      | *\"Write a realistic news article about X, sticking to confirmed facts.\"* |  \n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways (For a Hallucination-Free AI!)**  \n",
    "✅ **AI hallucinates** when it guesses, lacks data, or misinterprets prompts.  \n",
    "✅ **Fix it by:** Asking for sources, using constraints, and lowering temperature.  \n",
    "✅ **Best prompts** force AI to admit uncertainty and cite evidence.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8739c8d",
   "metadata": {},
   "source": [
    "# **Chat Models vs. Reasoning Models – Picking the Right AI Brain!**   \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Descriptive Differences (What They Are)**  \n",
    "\n",
    "### **Chat Models (The \"Friendly Conversationalist\")**  \n",
    "- Designed for **natural, flowing dialogue**  \n",
    "- Excels at:  \n",
    "  - Casual conversation  \n",
    "  - Answering general knowledge questions  \n",
    "  - Creative writing (stories, poems)  \n",
    "  - Summarizing text  \n",
    "- Examples: ChatGPT, Claude, Bard  \n",
    "\n",
    "### **Reasoning Models (The \"Math & Logic Genius\")**  \n",
    "- Optimized for **structured problem-solving**  \n",
    "- Excels at:  \n",
    "  - Step-by-step calculations  \n",
    "  - Code debugging  \n",
    "  - Mathematical proofs  \n",
    "  - Logical deductions  \n",
    "- Examples: GPT-4 with Code Interpreter, DeepMind's AlphaCode  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Characteristic Differences (How They Behave)**  \n",
    "\n",
    "| **Trait**               | **Chat Models**                | **Reasoning Models**            |  \n",
    "|-------------------------|-------------------------------|--------------------------------|  \n",
    "| **Response Style**       | Natural, conversational       | Precise, technical             |  \n",
    "| **Approach to Problems** | Intuitive, general            | Methodical, step-by-step       |  \n",
    "| **Strengths**           | Adaptability, creativity      | Accuracy, logical consistency  |  \n",
    "| **Weaknesses**          | May skip steps in reasoning   | Can sound robotic in conversation |  \n",
    "| **Best Temperature**    | 0.5-0.9 (balanced tone)      | 0.1-0.3 (high precision)       |  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. When to Use Which? (Comparison Table)**  \n",
    "\n",
    "| **Use Case**               | **Best Choice**               | **Why?**                      |  \n",
    "|---------------------------|------------------------------|-------------------------------|  \n",
    "| **Casual Conversation**    | Chat Model                   | More natural, engaging replies |  \n",
    "| **Customer Service Chat**  | Chat Model                   | Handles varied queries fluidly |  \n",
    "| **Debugging Python Code**  | Reasoning Model              | Follows logic precisely       |  \n",
    "| **Writing a Poem**        | Chat Model                   | Better creativity & flow      |  \n",
    "| **Solving Math Equations** | Reasoning Model              | Shows step-by-step work       |  \n",
    "| **Research Summarization** | Chat Model                   | Condenses info conversationally |  \n",
    "| **Legal Document Analysis**| Reasoning Model              | Extracts details accurately   |  \n",
    "| **Brainstorming Ideas**    | Chat Model                   | Generates more diverse concepts |  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebfe7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
